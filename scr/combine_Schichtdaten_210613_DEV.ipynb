{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation swissgeol.ch and map.geo.admin.ch\n",
    "\n",
    "# Borehole data\n",
    "\n",
    "#### Editor: ON; Date: 10.11.2020\n",
    "#### Modified: 13.6.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------- Let the Script begin ... ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import 'little helpers' for convenience\n",
    "import onUtils\n",
    "\n",
    "# for opening filedialog\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "import json\n",
    "\n",
    "# For Pandas\n",
    "#pd.set_option('max_columns', None)\n",
    "#pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coniguration read successfully\n"
     ]
    }
   ],
   "source": [
    "# Load Configuration\n",
    "with open(\"/Users/oesterli/Desktop/python_space/bh_prep/data/input/helper/config.json\", \"r\") as jsonfile:\n",
    "    conf = json.load(jsonfile)\n",
    "    print(\"Coniguration read successfully\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'Coniguration read successfully'\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "## timestamp for out_dir\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "## Convert number of run (run_num) to two digit str\n",
    "#run_dir = '_'.join(['run',now,str(run_num).zfill(2)])\n",
    "run_dir = '_'.join(['RUN',now])\n",
    "\n",
    "## Define output directory path\n",
    "out_dir_run = os.path.join(conf[\"root_dir\"],'data/output',run_dir)\n",
    "\n",
    "# Define name of log-file\n",
    "log_name = '_'.join(['LOG',now])\n",
    "\n",
    "## input directory for borhole data\n",
    "in_dir_bh = os.path.join(conf[\"root_dir\"], 'data/input/bh')\n",
    "\n",
    "## input directory for helper files\n",
    "in_dir_hp = os.path.join(conf[\"root_dir\"],'data/input/helper')\n",
    "\n",
    "## Define valide date range\n",
    "startD = '1700-01-01'\n",
    "endD = '2200-01-01'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_run_dir(f_out_dir_run):\n",
    "    '''\n",
    "    Check if run directory already exists. If no, create directory.\n",
    "    Parameter: run directory path\n",
    "    '''\n",
    "    # Check if out_dir already exists\n",
    "    if os.path.isdir(f_out_dir_run):\n",
    "        print('ok! Out directory already exists.')\n",
    "    else:\n",
    "        print('Output directory not existing, directory will de created')\n",
    "        os.mkdir(f_out_dir_run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data exported from GeODin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(f_in_dir_bh):\n",
    "    '''\n",
    "    Read input exel-files(.xlsx) from input directory. \n",
    "    Join all input files into one Pandas Dataframe.\n",
    "    Parameter: input directory of boreholes\n",
    "    '''\n",
    "    \n",
    "    # create global variable\n",
    "    global all_data\n",
    "    \n",
    "    # Read all .xlsx-Files from source directory\n",
    "    in_data = glob.glob(os.path.join(f_in_dir_bh,'*.xlsx'))\n",
    "    \n",
    "    # Read all data and combine it into one file. Continuos index will be created \n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for f in in_data:\n",
    "        df = pd.read_excel(f, date_parser=False, dtype={'BOHREDAT':str}) # IMPORTANT: deactive date paser und set BOHREDAT = str\n",
    "        all_data = all_data.append(df,ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting global variables\n",
    "#del globals()['all_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting data\n",
    "### Data types of selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dtypes of selected columns\n",
    "def check_dtypes():\n",
    "    '''Doc String'''\n",
    "    #Check dtypes of selected columns\n",
    "    print(\"Data types of selected data:\\n\",all_data[conf[\"sel_cols_2\"]].dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some simple statistics\n",
    "def create_stats():\n",
    "    # Create some simple statistics\n",
    "    num_bh = len(all_data['SHORTNAME'].unique())\n",
    "    num_layers = all_data.shape[0]\n",
    "    print('Number of individual borehole: ', num_bh)\n",
    "    print('Number of individual layers: ', num_layers)\n",
    "\n",
    "    # calculate number of layers per borehole\n",
    "    layer_per_bh = all_data.groupby('SHORTNAME')['DEPTHFROM'].nunique()\n",
    "    print(layer_per_bh.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and correct dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dates():\n",
    "    '''Doc String'''\n",
    "    # Check individual types of BOHREDAT\n",
    "    bohredatTypes = all_data['BOHREDAT'].apply(type).unique()\n",
    "\n",
    "    # Check max, min of dates \n",
    "    for i in bohredatTypes:\n",
    "\n",
    "        print('Max: ', i, all_data['BOHREDAT'].loc[all_data['BOHREDAT'].apply(type) == i].unique().max())\n",
    "        print('Min: ', i, all_data['BOHREDAT'].loc[all_data['BOHREDAT'].apply(type) == i].unique().min())\n",
    "    \n",
    "    # Replace wrong values and values out of bound\n",
    "    all_data['BOHREDAT_1'] = all_data['BOHREDAT'].apply(lambda x: '1968-06-17' if str(x) == '17.06.1668' else ('1700-01-01' if str(x) < startD else ('2200-01-01' if str(x) > endD else x)))\n",
    "\n",
    "    # convert column to datetime\n",
    "    all_data['BOHREDAT_1'] = pd.to_datetime(all_data['BOHREDAT_1'], infer_datetime_format=True, dayfirst=True, errors='raise')\n",
    "\n",
    "    # overwrite column\n",
    "    all_data['BOHREDAT'] = all_data['BOHREDAT_1']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data():\n",
    "    # Sort data by \"SHORTNAME\" and \"DEPTHFROM\"\n",
    "    \n",
    "    global all_data_sorted\n",
    "    \n",
    "    all_data_sorted = all_data.sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"])\n",
    "\n",
    "    # Create an index column\n",
    "    all_data_sorted['index'] = all_data_sorted.index\n",
    "\n",
    "    print('Shape of data set: ', all_data_sorted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_data():\n",
    "    # Round data\n",
    "    #df['DataFrame column'].round(decimals=number of decimal places needed)\n",
    "    all_data_sorted['XCOORD'] = all_data_sorted['XCOORD'].round(decimals=2)\n",
    "    all_data_sorted['YCOORD'] = all_data_sorted['YCOORD'].round(decimals=2)\n",
    "    all_data_sorted['ZCOORDB'] = all_data_sorted['ZCOORDB'].round(decimals=2)\n",
    "    all_data_sorted['TIEFEMD'] = all_data_sorted['TIEFEMD'].round(decimals=2)\n",
    "    all_data_sorted['DEPTHFROM'] = all_data_sorted['DEPTHFROM'].round(decimals=2)\n",
    "    all_data_sorted['DEPTHTO'] = all_data_sorted['DEPTHTO'].round(decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export raw data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_raw_data():\n",
    "    # Export all data to csv\n",
    "\n",
    "    bh_raw_file = '_'.join(['bh_raw', str(now)]) +'.csv'\n",
    "    bh_raw_path = os.path.join(out_dir_run, bh_raw_file)\n",
    "    all_data_sorted.to_csv(bh_raw_path,index=None, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h1>Run script</h1></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory not existing, directory will de created\n"
     ]
    }
   ],
   "source": [
    "# Call function \"make_run_dir()\"\n",
    "make_run_dir(out_dir_run)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'Run dir checked'\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function \"read_input()\"\n",
    "read_input(in_dir_bh)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'Number of inputfiles detected: ' + str(len(in_data)) + str(in_data)\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of selected data:\n",
      " XCOORD        float64\n",
      "YCOORD        float64\n",
      "ZCOORDB       float64\n",
      "ORIGNAME       object\n",
      "NAMEPUB        object\n",
      "SHORTNAME      object\n",
      "BOHREDAT       object\n",
      "GRUND          object\n",
      "RESTRICTIO     object\n",
      "TIEFEMD       float64\n",
      "DEPTHFROM     float64\n",
      "DEPTHTO       float64\n",
      "LAYERDESC      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Call function \"check_dtypes)()\"\n",
    "check_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  <class 'str'> 9999-09-09 00:00:00\n",
      "Min:  <class 'str'> 01.01.1111\n",
      "Max:  <class 'float'> nan\n",
      "Min:  <class 'float'> nan\n"
     ]
    }
   ],
   "source": [
    "# Call function \"correct_dates()\"\n",
    "correct_dates()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'all_data: ', all_data.shape\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individual borehole:  6060\n",
      "Number of individual layers:  85822\n",
      "count    6060.000000\n",
      "mean       14.159076\n",
      "std        17.686057\n",
      "min         1.000000\n",
      "25%         7.000000\n",
      "50%        11.000000\n",
      "75%        16.000000\n",
      "max       400.000000\n",
      "Name: DEPTHFROM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# call function \"create_stats()\"\n",
    "create_stats()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'Number of individual borehole: ', num_bh, '; Number of individual layers: ', num_layers\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  <class 'pandas._libs.tslibs.timestamps.Timestamp'> 2200-01-01T00:00:00.000000000\n",
      "Min:  <class 'pandas._libs.tslibs.timestamps.Timestamp'> 1700-01-01T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "correct_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data set:  (85822, 106)\n"
     ]
    }
   ],
   "source": [
    "sort_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'all_data_sorted with index: ', all_data_sorted.shape\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_raw_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'all_data_sorted exported to: ', bh_raw_path\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h1>^^^^^^^^^^^^^^^ bis hier</h1></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing PRIVATE data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catch all columns and column numbers in dict\n",
    "This shall help to easily see which cloumn is mentioned in warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10,13,17,18,20,71,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all columns name to list \"cols\"\n",
    "cols = all_data_sorted.columns.to_list()\n",
    "\n",
    "# Create a dictionry of column names and locations\n",
    "col_dicts = {}\n",
    "\n",
    "keys = len(cols)\n",
    "values = cols\n",
    "\n",
    "for i in range(keys):\n",
    "        col_dicts[i] = values[i]\n",
    "\n",
    "print(col_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the shape (rows, columns) of the combined files\n",
    "all_data_sorted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for duplicated rows  and eliminate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns except the index column, in order to use it as a subset for finding duplicate rows\n",
    "cols_2 = cols[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate rows (checking duplicates based on all columns)\n",
    "dup_rows = all_data_sorted[all_data_sorted.duplicated(subset=cols_2)]\n",
    "print(\"Duplicate Rows except first occurrence based on all columns are :\", dup_rows['LONGNAME'].count())\n",
    "\n",
    "#Show duplicate entries if exisiting\n",
    "if len(dup_rows) > 0:\n",
    "    dup_rows.head()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = \"Duplicate Rows except first occurrence based on all columns are :\", dup_rows['LONGNAME'].count()\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop all duplicated rows and save result to \"all_data_unique\"\n",
    "if len(dup_rows) > 0:\n",
    "    all_data_unique = all_data_sorted.drop_duplicates(subset=cols_2)\n",
    "    print(len(dup_rows), \" dropped\")\n",
    "    print(\"shape all_data_unipue: \", all_data_unique.shape)\n",
    "    print(\"shape all_data_sorted: \", all_data_sorted.shape)\n",
    "else:\n",
    "    all_data_unique = all_data_sorted\n",
    "    print(\"No rows dropped\")\n",
    "    #print(\"shape all_data_unipue: \", all_data_unique.shape)\n",
    "    print(\"shape all_data_sorted: \", all_data_sorted.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = dup_rows['LONGNAME'].count(), \"rows dropped\"\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant columns and save into \"all_data_unique_sel\"\n",
    "all_data_unique_sel = all_data_unique[sel_cols]\n",
    "\n",
    "# For entire data set with all columns: Save all into \"all_data_unique_sel\"\n",
    "#all_data_unique_sel = all_data_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_unique_sel.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt =  \"Columns selected: \", sel_cols\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> > Create GDF for all data PRIVATE data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe \"all_data_unique_sel\" to geodataframe \"gdf\" and set inital crs to epsg:2056 \n",
    "geom = [Point(xy) for xy in zip(all_data_unique_sel.XCOORD, all_data_unique_sel.YCOORD)]\n",
    "crs = {\"init\" : \"epsg:2056\"}\n",
    "private_bh = gpd.GeoDataFrame(all_data_unique_sel, crs=crs, geometry=geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gdf\n",
    "private_bh.head()\n",
    "#private_bh.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt =  \"GDF create for PRIVATE BH with shape: \", private_bh.shape\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot GDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load swiss boundary from shapefile\n",
    "ch_perimeter = gpd.read_file('./data/input/helper/swissBOUNDARIES3D_1_3_TLM_LANDESGEBIET.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only swiss perimeter\n",
    "ch_perimeter = ch_perimeter[ch_perimeter['ICC'] == 'CH']\n",
    "ch_perimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create buffer around CH-Perimeter\n",
    "ch_peri_buf_10km = ch_perimeter.buffer(20000) # 20km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GeoDataFrame together with swiss boundary \n",
    "base = ch_perimeter.plot(color='white', edgecolor='black')\n",
    "lyr_1 = ch_peri_buf_10km.plot(ax=base, edgecolor='blue', facecolor='none')\n",
    "private_bh.plot(ax=lyr_1, color='red', markersize=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP GDF to swiss boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip the data using GeoPandas clip\n",
    "gdf_clip = gpd.clip(gdf, ch_perimeter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt =  \"GDF clipped to swiss boundary\"\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject EPSG:2056 to EPSG:4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write re-projected coordinates into new column\n",
    "#gdf_trans = gdf\n",
    "private_bh[\"geom4326\"] = private_bh[\"geometry\"].to_crs(\"epsg:4326\")\n",
    "\n",
    "private_bh[\"x4326\"] = private_bh[\"geom4326\"].apply(lambda p: p.x)\n",
    "private_bh[\"y4326\"] = private_bh[\"geom4326\"].apply(lambda p: p.y)\n",
    "\n",
    "private_bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_bh.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt =  \"GDF re-projected: \", \n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns according to conventions\n",
    "private_bh.columns = export_pri_cols\n",
    "private_bh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use private_bh as basis for creating OPEN data set and 2D data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Export PRIVATE data to file >>>>> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export PRIVATE data to csv\n",
    "#all_data_sorted.to_csv(\"./data/output/Schichtdaten/bh_raw_201109.csv\",index=None)\n",
    "bh_private_file = '_'.join(['bh_private', str(now), str(run_num).zfill(2),]) +'.csv'\n",
    "bh_private_path = os.path.join(out_dir_run,bh_private_file)\n",
    "private_bh.to_csv(bh_private_path,index=None, sep=';')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'PRIVATE boreholes exported to: ', bh_private_path\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data for OPEN dataset\n",
    "### Introduce columns \"start\" and \"end\" to to store start and end depth of entire borehole to each row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset columns names to original\n",
    "private_bh.columns = cols_pub\n",
    "private_bh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns \"start\" and \"end\" and fill it with start and end depth\n",
    "private_bh['start'] = private_bh.groupby('SHORTNAME')['DEPTHFROM'].transform('min')\n",
    "private_bh['end'] = private_bh.groupby('SHORTNAME')['DEPTHTO'].transform('max')\n",
    "\n",
    "# show head\n",
    "private_bh.sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"]).head(10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'Columns \"start\" and \"end\" added.', private_bh.columns\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split layer based on restriction level\n",
    "Restriction level:\n",
    "* g = restricted\n",
    "* b = restricted until date specified\n",
    "* f = free, not restricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select row which have \"RESTRICTIO\" == \"g\"\n",
    "# Select row which have \"RESTRICTIO\" == \"f\"\n",
    "# Select row which have \"RESTRICTIO\" == \"b\"\n",
    "\n",
    "df_g = private_bh.loc[private_bh[\"RESTRICTIO\"] == \"g\"].sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"])\n",
    "df_f = private_bh.loc[private_bh[\"RESTRICTIO\"] == \"f\"].sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"])\n",
    "df_b = private_bh.loc[private_bh[\"RESTRICTIO\"] == \"b\"].sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"])\n",
    "\n",
    "# Sum of all rows from df_g, df_b, df_f\n",
    "df_sum = df_g.shape[0] + df_b.shape[0] + df_f.shape[0]\n",
    "\n",
    "print(\"All unique data: \",private_bh.shape)\n",
    "print(\"Restricted unique data: \", df_g.shape)\n",
    "print(\"Restricted until unique data: \", df_b.shape)\n",
    "print(\"Non-restricted unique data: \", df_f.shape)\n",
    "print(\"Restricted + restircted until + Non-restricted data: \", df_sum)\n",
    "print(\"Difference between all unique data and restriceted + Non-restricted: \", private_bh.shape[0] - df_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete duplicates in df_g\n",
    "df_g_unique = private_bh.loc[private_bh[\"RESTRICTIO\"] == \"g\"].sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"]).drop_duplicates(\"SHORTNAME\")\n",
    "print('df_g_unique: ', df_g_unique.shape)\n",
    "\n",
    "# delete duplicates in df_b\n",
    "df_b_unique = private_bh.loc[private_bh[\"RESTRICTIO\"] == \"b\"].sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"]).drop_duplicates(\"SHORTNAME\")\n",
    "print('df_b_unique: ', df_b_unique.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g_unique.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = \"All unique data: \",private_bh.shape, '; \"g\" unique data: ', df_g.shape, '; \"b\" unique data: ', df_b.shape, '; \"f\" unique data: ', df_f.shape, '; Difference: ', private_bh.shape[0] - df_sum \n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append df_b to df_g, so that all restriced data are in one dataframe\n",
    "df_g_unique = df_g_unique.append(df_b_unique)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = \"All restricted unique data combined: \",private_bh.shape, '; \"g\" unique data: ', df_g.shape, '; \"b\" unique data: ', df_b.shape, '; \"f\" unique data: ', df_f.shape, '; Difference: ', private_bh.shape[0] - df_sum \n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g_unique.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace layer details with undefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite \"DEPTHFROM\" and \"DEPTHTO\" with \"start\" and \"end\", respectively\n",
    "df_g_unique[\"DEPTHFROM\"] = df_g_unique[\"start\"]\n",
    "df_g_unique[\"DEPTHTO\"] = df_g_unique[\"end\"]\n",
    "\n",
    "# Overwrite \"LAYERDESC\" of restricted boreholes (\"RESTRICTIO = \"g) with \"Undefined\"\n",
    "df_g_unique[[\"LAYERDESC\", \"ORIGGEOL\", \"ORIGNAME\", \"BOHRTYP\", \"CHRONOSTR\", \"LITHOLOGY\", \"LITHOSTRAT\"]] = \"Undefined\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = \"Layer details replaced with undefined values \" \n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine free and restricted data into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframe df_g and df_f\n",
    "frames = [df_g_unique, df_f]\n",
    "open_bh = pd.concat(frames)\n",
    "\n",
    "# Drop the columns \"start\" and \"end\"\n",
    "open_bh = open_bh.drop([\"start\", \"end\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataset by unique-ID \"SHORTNAME\" and \"DEPTHFROM\"\n",
    "open_bh = open_bh.sort_values(by=[\"SHORTNAME\", \"DEPTHFROM\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = \"Data sets re-combined, rows sorted by 'SHORTNAME' and 'DEPTHFROM'.\" \n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_bh['SHORTNAME'].nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = \"Open data created: \", open_bh['SHORTNAME'].nunique()\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_bh.columns = export_pub_cols\n",
    "open_bh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Export OPEN data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export PRIVATE data to csv\n",
    "#all_data_sorted.to_csv(\"./data/output/Schichtdaten/bh_raw_201109.csv\",index=None)\n",
    "bh_open_file = '_'.join(['bh_open', str(now), str(run_num).zfill(2),]) +'.csv'\n",
    "bh_open_path = os.path.join(out_dir_run,bh_open_file)\n",
    "open_bh.to_csv(bh_open_path,index=None, sep=';')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'OPEN boreholes exported to: ', bh_open_path\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing 2D Data\n",
    "### Create 2D data set by aggregating records by \"SHORTNAME\", each record becomes one borehole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset columns names to orginal\n",
    "open_bh.columns = cols_pub\n",
    "open_bh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all duplicated record based on unique-ID \"SHORTNAME\" apart from first record\n",
    "\n",
    "bh_2d = open_bh.drop_duplicates(subset=[\"SHORTNAME\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_2d.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'All duplicated rows with same \"SHORTNAME\" deleted: '\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only relevant columns for 2D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relvant columns and save it back into \"df_all_unique\"\n",
    "#df_all_unique = df_all_unique[[\"index\",\"XCOORD\",\"YCOORD\",\"ZCOORDB\",\"ORIGNAME\",\"NAMEPUB\",\"SHORTNAME\", 'BOHREDAT',\"BOHRTYP\", 'GRUND',\"RESTRICTIO\",\"TIEFEMD\"]]\n",
    "bh_2d = bh_2d[cols_2D]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'Relevant columns for 2D-data selected', bh_2d.columns\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new Atribute for Link to swissgeol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameter \n",
    "baseURL = 'https://swissgeol.ch/?'\n",
    "para_sep = '&'\n",
    "\n",
    "layer_key = 'layers='\n",
    "layer_value = 'boreholes'\n",
    "\n",
    "layer_vis = 'layers_visibility='\n",
    "layer_vis_value = 'true'\n",
    "\n",
    "layer_trans = 'layers_transparency='\n",
    "layer_trans_value = '0'\n",
    "link_key = 'zoom_to='\n",
    "link_sep = ','\n",
    "\n",
    "# create Link\n",
    "#df_all_unique['link'] = baseURL + layer_key + layer_value + para_sep + layer_vis + layer_vis_value + para_sep + layer_trans + layer_trans_value + para_sep + link_key + df_all_unique['x4326'].map(str) + link_sep + df_all_unique['y4326'].map(str) +  link_sep + df_all_unique['ZCOORDB'].map(str)\n",
    "bh_2d['link'] = baseURL + layer_key + layer_value + para_sep + layer_vis + layer_vis_value + para_sep + layer_trans + layer_trans_value + para_sep + link_key + bh_2d['x4326'].map(str) + link_sep + bh_2d['y4326'].map(str) +  link_sep + '0'\n",
    "\n",
    "bh_2d.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = 'Link to 3D-object created'\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bh_2d['link'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort how many unique record are in the dataset\n",
    "\n",
    "bh_2d = bh_2d.sort_values(by=[\"SHORTNAME\"])\n",
    "bh_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Export 2D data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export PRIVATE data to csv\n",
    "\n",
    "bh_2d_file = '_'.join(['bh_2D', str(now), str(run_num).zfill(2),]) +'.csv'\n",
    "bh_2d_path = os.path.join(out_dir_run,bh_2d_file)\n",
    "bh_2d.to_csv(bh_2d_path,index=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# checkpoint\n",
    "check_txt = '2D-data exported \\n ------------------------'\n",
    "onUtils.logger_3(check_txt, fname=log_name, fpath=out_dir_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env02",
   "language": "python",
   "name": "env02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
